{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.- Compara el modelo anterior con un modelo que use convoluciones 2D. Ten en cuenta que has de reordenar los datos para poder usar convoluciones 2D. Compara el resultado en unos datos de test con el resultado del modelo anterior. ¿Obtienes mejoras al utilizar las convoluciones 2D?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Layer, Dense, GRU, Conv2D, Flatten, Dropout, BatchNormalization, MaxPooling2D, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leemos los datos de apple\n",
    "apple = pd.read_csv(r\"C:\\Users\\Carlos\\Documents\\MIA-X\\Modulo 4\\Machine Learning\\Redes Convulcionales\\CNN + RNN\\practica_rnn_cnn\\Stocks\\aa.us.txt\", delimiter=\",\")\n",
    "amazon = pd.read_csv(r\"C:\\Users\\Carlos\\Documents\\MIA-X\\Modulo 4\\Machine Learning\\Redes Convulcionales\\CNN + RNN\\practica_rnn_cnn\\Stocks\\amzn.us.txt\", delimiter=\",\")\n",
    "microsoft = pd.read_csv(r\"C:\\Users\\Carlos\\Documents\\MIA-X\\Modulo 4\\Machine Learning\\Redes Convulcionales\\CNN + RNN\\practica_rnn_cnn\\Stocks\\msft.us.txt\", delimiter=\",\")\n",
    "facebook = pd.read_csv(r\"C:\\Users\\Carlos\\Documents\\MIA-X\\Modulo 4\\Machine Learning\\Redes Convulcionales\\CNN + RNN\\practica_rnn_cnn\\Stocks\\fb.us.txt\", delimiter=\",\")\n",
    "google = pd.read_csv(r\"C:\\Users\\Carlos\\Documents\\MIA-X\\Modulo 4\\Machine Learning\\Redes Convulcionales\\CNN + RNN\\practica_rnn_cnn\\Stocks\\googl.us.txt\", delimiter=\",\")\n",
    "#Me quedo con los datos de apertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos la columna Date como indice\n",
    "for asset in [apple, amazon, microsoft, google, facebook]:\n",
    "    asset.index = pd.to_datetime(asset.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtramos por el indice para homogeneizar los datos\n",
    "data_close = apple[facebook.index[0]:].Close\n",
    "apple = apple[facebook.index[0]:].Open\n",
    "amazon = amazon[facebook.index[0]:].Open\n",
    "microsoft = microsoft[facebook.index[0]:].Open\n",
    "google = google[facebook.index[0]:].Open\n",
    "facebook = facebook.Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Me genero una matriz de ceros con las dimensiones que quiero, n_ejemplos x 5 datos. \n",
    "LAG = 5\n",
    "data = np.zeros((apple.shape[0]- LAG, LAG, 5))\n",
    "close = np.zeros((data_close.shape[0]- LAG, LAG, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unimos los datos en un único dataframe\n",
    "for i in range(0,(apple.shape[0] - LAG)):\n",
    "    data[i,:,0] = apple.iloc[i:i+LAG]\n",
    "    data[i,:,1] = amazon.iloc[i:i+LAG]\n",
    "    data[i,:,2] = microsoft.iloc[i:i+LAG]\n",
    "    data[i,:,3] = google.iloc[i:i+LAG]\n",
    "    data[i,:,4] = facebook.iloc[i:i+LAG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:round(data.shape[0]*0.7),:-1,:]\n",
    "x_test = data[round(data.shape[0]*0.7):,:-1,:]\n",
    "y_train = close[:round(data.shape[0]*0.7),-1,0]\n",
    "y_test = close[round(data.shape[0]*0.7):,-1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder utilizar capas convulcionales 2D es necesario reordenar nuestros datos para que tengan la dimensión del canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reordenamos los datos para poder usar convulciones 2D\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(963, 4, 5, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 4, 5, 50)          500       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4, 5, 50)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 2, 2, 50)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 2, 2, 30)          13530     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2, 2, 30)          0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 1, 1, 30)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 30)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,061\n",
      "Trainable params: 14,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Definimos un modelo con una capa convolucional 2D y 1 capa densa\n",
    "model = Sequential()\n",
    "model.add(Conv2D(50, 3, input_shape=(4,5,1), padding='same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Conv2D(30, 3, padding='same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# mostramos el modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 27.8849\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 17.2842\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 13.9536\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 11.5457\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 10.0842\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6657\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4142\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 5.6603\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 5.6493\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 4.9127\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 4.6523\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 3.9431\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 3.8357\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 3.5670\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 3.5021\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.8492\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.5372\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.5539\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.2176\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.1622\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.8721\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.7819\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.6349\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.6350\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.6149\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.3299\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.3760\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.2529\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.1064\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.0809\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.9840\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.1276\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 1.0020\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.8842\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.8725\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.7724\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.7292\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.6972\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.6972\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.6070\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.6491\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.6261\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5494\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5732\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4825\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5260\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4822\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4446\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4552\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4226\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,  epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 1ms/step - loss: 0.5583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5583410263061523"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo comparamos con el resultado del ejercicio 7, vemos como el modelo anterior obtuvo un resultado bastante más favorable, con un erroor de 0.19. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11804bb1f250a85d0267cdde9a53e916113451a0f88a057835fc51e3493d4141"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
